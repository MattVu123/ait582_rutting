{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e740020-0739-4d4b-a9ef-5cbfb68ba8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Processing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfbeda92-dad5-4e8a-83bd-05d3cc6588c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This juypter notebook processes the data from dbfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8e6c30e-7867-473f-94f2-47dca5496681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## This section imports the raw datasets and pre-processed them for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecd94cf-8de4-4565-a329-a3328e6729ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Initialize Spark session\n",
    "# -----------------------------------------------\n",
    "spark = SparkSession.builder.appName(\"SouthernRuttingPreprocessing\").getOrCreate()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Define base path\n",
    "# -----------------------------------------------\n",
    "base_path = \"/Volumes/workspace/mlrutting-3/mlrutting-3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a798b2bd-0ab0-486c-9df9-890ec00e60c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 1: Load CSV datasets\n",
    "# -----------------------------------------------\n",
    "humidity_df   = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(base_path + \"humidity.csv\")\n",
    "precip_df     = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(base_path + \"precipitation.csv\")\n",
    "rutting_df    = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(base_path + \"rutting.csv\")\n",
    "solar_df      = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(base_path + \"solar.csv\")\n",
    "temp_df       = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(base_path + \"temp.csv\")\n",
    "traffic_df    = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(base_path + \"traffic_volume.csv\")\n",
    "wind_df       = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(base_path + \"wind.csv\")\n",
    "grid_df       = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(base_path + \"merra_grid_section.csv\")\n",
    "\n",
    "print(\"All CSVs loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "010e7d3c-13a2-4ed6-985d-88db1ea4c049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 2: Keep only COUNT5–COUNT13 in traffic data (remove CODE01–CODE04)\n",
    "# -----------------------------------------------\n",
    "count_cols = [f\"COUNT{i}\" for i in range(5, 14)]\n",
    "\n",
    "# Drop CODE01–CODE04 if they exist\n",
    "traffic_df = traffic_df.drop(*[f\"COUNT{i:02d}\" for i in range(1, 5)])\n",
    "\n",
    "print(\"Filtered traffic_volume.csv to include only COUNT5–COUNT13 (removed CODE01–CODE04)\")\n",
    "traffic_df.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4157603f-6630-4d47-83dc-07c8c508ee03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 3: Merge all climate-related datasets\n",
    "# -----------------------------------------------\n",
    "climate_dfs = [humidity_df, precip_df, solar_df, temp_df, wind_df,grid_df]\n",
    "\n",
    "merged_climate = reduce(\n",
    "    lambda left, right: left.join(right, on=[\"MERRA_ID\", \"YEAR\", \"MONTH\"], how=\"outer\"),\n",
    "    climate_dfs\n",
    ")\n",
    "print(\"✅ Climate datasets merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca94b105-bcc2-493e-9410-0f1d83e10021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 4: Merge climate + traffic datasets\n",
    "# -----------------------------------------------\n",
    "merged_climate_traffic = merged_climate.join(\n",
    "    traffic_df, on=[\"SHRP_ID\", \"YEAR\", \"MONTH\"], how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"✅ Added traffic volume data (COUNT5–COUNT13)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8927f3-b528-4454-b2aa-0854eca7c607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 5: Merge with MERRA grid section to attach SHRP_ID, state, lat/lon, elevation\n",
    "# -----------------------------------------------\n",
    "merged_climate_traffic = merged_climate_traffic.join(\n",
    "    grid_df, on=\"MERRA_ID\", how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"✅ Added SHRP_ID and grid info to climate + traffic data\")\n",
    "\n",
    "# Now the merged dataset has SHRP_ID — use that as the join key for rutting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b5c212-cf66-4d40-8cb9-131927bf70bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 5: Prepare rutting dataset (extract YEAR, MONTH)\n",
    "# -----------------------------------------------\n",
    "rutting_df = rutting_df.withColumn(\"Survey_Date\", F.to_timestamp(\"Survey_Date\"))\n",
    "rutting_df = rutting_df.withColumn(\"YEAR\", F.year(\"Survey_Date\"))\n",
    "rutting_df = rutting_df.withColumn(\"MONTH\", F.month(\"Survey_Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595ded20-eaf6-4794-b268-ac89682f47fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 6: Merge with rutting data using SHRP_ID, YEAR, and MONTH\n",
    "# -----------------------------------------------\n",
    "rutting_df = rutting_df.withColumn(\"Survey_Date\", F.to_timestamp(\"Survey_Date\"))\n",
    "rutting_df = rutting_df.withColumn(\"YEAR\", F.year(\"Survey_Date\"))\n",
    "rutting_df = rutting_df.withColumn(\"MONTH\", F.month(\"Survey_Date\"))\n",
    "\n",
    "final_df = rutting_df.join(\n",
    "    merged_climate_traffic,\n",
    "    on=[\"SHRP_ID\", \"YEAR\", \"MONTH\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"✅ Final merged dataset (Rutting + Climate + Traffic + Grid)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b6c853-a380-4481-90f6-e41f72d320f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 6: Merge everything together\n",
    "# -----------------------------------------------\n",
    "final_df = rutting_df.join(\n",
    "    merged_climate_traffic,\n",
    "    on=[\"MERRA_ID\",\"SHRP_ID\", \"YEAR\", \"MONTH\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"✅ All datasets merged into final DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44058fc8-b0d6-45fb-b9db-a7250fdc68bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 7: Filter by Southern states\n",
    "# -----------------------------------------------\n",
    "southern_states = [\n",
    "    \"DE\", \"MD\", \"DC\", \"VA\", \"WV\",\n",
    "    \"KY\", \"TN\", \"NC\", \"SC\", \"GA\", \"FL\",\n",
    "    \"AL\", \"MS\", \"AR\", \"LA\", \"OK\", \"TX\"\n",
    "]\n",
    "\n",
    "# Ensure STATE column exists before filtering\n",
    "if \"STATE\" in final_df.columns:\n",
    "    final_df = final_df.filter(F.col(\"STATE\").isin(southern_states))\n",
    "    print(\"✅ Filtered dataset to include only Southern states\")\n",
    "else:\n",
    "    print(\"⚠️ No STATE column found — skipping Southern state filtering\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5590460442768906,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "processing.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
